# -*- coding: utf-8 -*-
"""tf-keras-housingprices-regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pz4KaKNMSrIkrKo9Q03gGkQf9U9FPZQY
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('kc_house_data.csv')
# kaggle house sales prediction dataset

df.describe().transpose()

plt.figure(figsize=(10,6))
sns.displot(df['price'])

sns.countplot(df['bedrooms'])

df.corr()['price'].sort_values()
# correlation with the price; we can analyze this further

sns.scatterplot(x='price',y='sqft_living',data=df)

sns.boxplot(x='bedrooms',y='price',data=df)

plt.figure(figsize=(20,20))
sns.scatterplot(x='price',y='long',data=df)

plt.figure(figsize=(20,20))
sns.scatterplot(x='price',y='lat',data=df)
# at a certain combination of lat - long there is an expensive housing area

plt.figure(figsize=(20,20))
sns.scatterplot(x='long',y='lat',data=df,hue='price')
# matxhing these results in a map

df.sort_values('price',ascending=False).head(20) # top 20 most expensive houses

# sample out the top 1% of the houses by price to clear up the map
non_top_1_percent = df.sort_values('price',ascending=False).iloc[216:]
plt.figure(figsize=(12,8))
sns.scatterplot(x='long',y='lat',data=non_top_1_percent,alpha=.45,palette='RdYlGn',hue='price')

sns.boxplot(x='waterfront',y='price',data=df)

# dropping attributes which are not relevant
df = df.drop('id',axis=1)

df['date'] = pd.to_datetime(df['date']) # converting date string into date datetime
df['date']

df['year'] = df['date'].apply(lambda date: date.year)
df['month'] = df['date'].apply(lambda date: date.month)

df.head(1)

plt.figure(figsize=(10,6))
sns.boxplot(x='month',y='price',data=df)

df.groupby('month').mean()['price'].plot()

df.groupby('year').mean()['price'].plot()

# df = df.drop('date',axis=1) <= ezt mÃ¡r droppoltam
df = df.drop('zipcode',axis=1) # we could also map these by price, but it requires domain experience

df['yr_renovated'].value_counts()
# 0 is an indicator of no renovation; we may categorize on this attribute

df['sqft_basement'].value_counts()
# 0 is an indicator, that there is no basement

"""**Data preprocessing**"""

from sklearn.model_selection import train_test_split

#X = df.drop('price',axis=1).values
#y = df['price'].values

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,random_state=101)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)

X_test = scaler.transform(X_test)

"""**Tensorflow**"""

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Dense

model = Sequential()

model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))

model.add(Dense(1))

model.compile(optimizer='adam',loss='mse')

model.fit(x=X_train,y=y_train,validation_data=(X_test,y_test),batch_size=128,epochs=400)
# batch size should be powers of 2; small batch sizes result in less overfitting

"""**Model performance evaluation**"""

losses = pd.DataFrame(model.history.history)

losses.plot()
# increasing validation loss would indicate overfitting

from sklearn.metrics import mean_absolute_error,mean_squared_error,explained_variance_score

predictions = model.predict(X_test)

mean_absolute_error(y_test,predictions)

mean_squared_error(y_test,predictions)

np.sqrt(mean_squared_error(y_test,predictions))

explained_variance_score(y_test,predictions)

plt.scatter(y_test,predictions)
plt.plot(y_test,y_test,'r')